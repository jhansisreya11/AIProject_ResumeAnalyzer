# -*- coding: utf-8 -*-
"""AIprojectFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1atXTneteS0qHmoeU7ZbEca0LcY4_h7X8
"""

!pip install PyPDF2

!pip3 install ftfy

import nltk
import csv

import os
import re
import PyPDF2
import spacy
import gensim.downloader as api

nltk.download('stopwords')

import numpy as np
import pandas as pd

from collections import Counter

from nltk.corpus import stopwords
stopw  = set(stopwords.words('english'))

import re
from ftfy import fix_text

df =pd.read_csv('/content/modified_jobs.csv')

df['test']=df['Job Description'].apply(lambda x: ' '.join([word for word in str(x).split() if len(word)>2 and word not in (stopw)]))
df['test']

df.head(5)

nlp = spacy.load("en_core_web_sm")

word_vectors = api.load("glove-wiki-gigaword-300")

import re
import PyPDF2
from gensim.models import KeyedVectors

"""### **EXTRACTING SKILLS FROM RESUME**"""

def extract_skills(text):
    if isinstance(text, str) or isinstance(text, bytes):
        skills = []
        skills_section = re.search(r'SKILLS\s*(.*?)(?=\n\n|$)', text, re.DOTALL)
        if skills_section:
            skills_text = skills_section.group(1)
            skills = [skill.strip() for skill in skills_text.split('\n')]
        return skills
    else:
        return []

resume_file = input('specify the path of the resume (format(.txt, .docx and .pdf))==')
with open(resume_file, 'rb') as file:
    pdf_reader = PyPDF2.PdfReader(file)
    text = ''
    for page in range(len(pdf_reader.pages)):
        text += pdf_reader.pages[page].extract_text()

print(text)

skills1 = extract_skills(text)

print(skills1)

def get_related_terms(term, topn=5):
    try:
        related_terms = word_vectors.most_similar(positive=[term], topn=topn)
        return [term for term, _ in related_terms]
    except KeyError:
        return []

def extract_skills_from_projects(projects_text):

    nlp = spacy.load("en_core_web_sm")


    doc = nlp(' '.join(projects_text))


    extracted_skills = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN']]


    word_counts = Counter(extracted_skills)

    extracted_skills = [word for word, count in word_counts.most_common() if count > 1]

    return extracted_skills

def expand_skills_with_related_terms(skills):
    expanded_skills = []
    for skill in skills:
        expanded_skills.append(skill)
        related_terms = get_related_terms(skill)
        expanded_skills.extend(related_terms)
    return expanded_skills

project_text = skills1

extracted_skills = extract_skills_from_projects(project_text)

def extract_skills_byComparing(text):
    if isinstance(text, str) or isinstance(text, bytes):
        skills = []

        with open('combined_skills.csv', newline='') as csvfile:
            reader = csv.reader(csvfile)
            skill_rows = [row[0].lower() for row in reader]


        section_titles = ['SKILLS', 'Technical Skills', 'personal projects', 'Expertise',
                          'Experience', 'Projects', 'Certifications', 'Achievements',
                          'Education', 'Courses']
        section_titles = [title.lower() for title in section_titles]

        pattern = '|'.join(section_titles)


        sections = re.finditer(r'({})\s*(.*?)(?=\n\n|$)'.format(pattern), text, re.IGNORECASE | re.DOTALL)
        for section in sections:
            section_text = section.group(2).lower()

            for row in skill_rows:
                if row in section_text:
                    skills.append(row)


        unique_skills = list(set(skills))
        return unique_skills
    else:
        return []

skills2 = extract_skills_byComparing(text)

skills2 = [skill.lower() for skill in skills2]
skills2 = list(set(skills2))

extracted_skills = [skill.lower() for skill in extracted_skills]

combined_skills = list(set(skills2 + extracted_skills))

print("Combined skills:")
print(combined_skills)

skills = []
for skill in combined_skills:
    skills.append(skill)
    skills.extend(get_related_terms(skill))

print(skills)

"""### **COMPARSION OF RESUME SKILLS WITH THE JOB DESCRIPTION SKILLS**"""

from collections import Counter

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

nltk.download('punkt')
nltk.download('stopwords')

def extract_skills_from_csv(csv_file):
    skills_set = set()
    with open(csv_file, 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            skills_set.add(row[0].strip().lower())
    return skills_set

def extract_skills_from_resume(resume_text, skills_set):
    resume_words = word_tokenize(resume_text.lower())
    stop_words = set(stopwords.words('english'))

    extracted_skills = []

    for word in resume_words:
        if word not in stop_words and word in skills_set:
            extracted_skills.append(word)

    return extracted_skills

skills_file = "/content/combined_skills.csv"
skills_set = extract_skills_from_csv(skills_file)

jobs_df = pd.read_csv("/content/modified_jobs.csv")

job_skills = []
total_skills_count = []

for idx, row in jobs_df.iterrows():
    job_description = row['Job Description']
    skill = extract_skills_from_resume(job_description, skills_set)
    unique_skills = set(skill)
    total_skills = len(unique_skills)
    job_skills.append(", ".join(unique_skills))
    total_skills_count.append(total_skills)

jobs_df['Job Skills'] = job_skills
jobs_df['Total Skills'] = total_skills_count

print(jobs_df['Job Skills'])
print(jobs_df['Total Skills'])

resume_skills_set = set(skills)

print(resume_skills_set)

resume_skills_set = {skill.lower().strip() for skill in resume_skills_set}

job_matched_skills_counter = {}
job_matched_skills = {}
matched_skills={}
matched_count = []

for idx, row in jobs_df.iterrows():

    job_skills_list = [skill.lower().strip() for skill in row['Job Skills'].split(", ")]

    job_skills_unique = set(job_skills_list)


    matched_skills = [skill for skill in job_skills_unique if skill.lower() in resume_skills_set]
    matched_count = len(matched_skills)


    job_matched_skills_counter[idx] = matched_count
    job_matched_skills[idx] = matched_skills

job_ids = []
job_titles = []
work_types = []
total_skills_counts = []
matched_skills_counts = []
percentage_matches = []
job_roles = []

top_matched_jobs = sorted(job_matched_skills_counter.items(), key=lambda x: x[1], reverse=True)[:200]

for job_id, matched_count in top_matched_jobs:
    if job_id in jobs_df['Job Id'].values:

        job_details = jobs_df[jobs_df['Job Id'] == job_id + 1].iloc[0]
        job_title = job_details['Job Title']
        work_type = job_details['Work Type']
        total_skills_count = len(job_details['Job Skills'].split(","))
        job_role = job_details['Role']


        percentage_match = (matched_count / total_skills_count) * 100


        job_ids.append(job_id + 1)
        job_titles.append(job_title)
        work_types.append(work_type)
        job_roles.append(job_role)

        total_skills_counts.append(total_skills_count)
        matched_skills_counts.append(matched_count)
        percentage_matches.append(percentage_match)



job_details_df = pd.DataFrame({
    'Job ID': job_ids,
    'Job Title': job_titles,
    'Work Type': work_types,
    'Job Role': job_roles,
    'Total Skills Count': total_skills_counts,
    'Matched Skills Count': matched_skills_counts,
    'Percentage Match': percentage_matches
})


job_details_df_sorted = job_details_df.sort_values(by='Percentage Match', ascending=False)


print(job_details_df_sorted)

"""### **PIE CHART BETWEEN JOB TITLE AND PERCENTAGE MATCH**"""

job_titles = job_details_df_sorted['Job Title']
total_skills_count = job_details_df_sorted['Percentage Match']

import pandas as pd
import matplotlib.pyplot as plt

grouped_skills_df = job_details_df_sorted.groupby('Job Title')['Percentage Match'].sum().reset_index()

job_titles = grouped_skills_df['Job Title']
total_skills_count = grouped_skills_df['Percentage Match']

plt.figure(figsize=(8, 8))
plt.pie(total_skills_count, labels=job_titles, autopct='%1.1f%%', startangle=140)
plt.axis('equal')
plt.title('Distribution of Percentage Match')
plt.show()

"""### **BAR CHART BETWEEN JOB TITLE AND TOTAL SKILLS COUNT**"""

import matplotlib.pyplot as plt

job_titles = job_details_df_sorted['Job Title']
total_skills_count = job_details_df_sorted['Total Skills Count']

plt.figure(figsize=(10, 6))
plt.bar(job_titles, total_skills_count, color='skyblue')
plt.xlabel('Job Title')
plt.ylabel('Total Skills Count')
plt.title('Total Skills Count by Job Title')
plt.xticks(rotation=90)
plt.show()

"""### **BAR CHART BETWEEN JOB TITLE AND MATCHED SKILLS COUNT**"""

import matplotlib.pyplot as plt

job_titles = job_details_df_sorted['Job Title']
matched_skills_count = job_details_df_sorted['Matched Skills Count']

plt.figure(figsize=(10, 6))
plt.bar(job_titles, matched_skills_count, color='lightgreen')
plt.xlabel('Job Title')
plt.ylabel('Matched Skills Count')
plt.title('Matched Skills Count by Job Title')
plt.xticks(rotation=90)
plt.show()

"""### **BAR CHART BETWEEN JOB TITLE AND PERCENTAGE MATCH**"""

import matplotlib.pyplot as plt

job_titles = job_details_df_sorted['Job Title']
percentage_match = job_details_df_sorted['Percentage Match']

plt.figure(figsize=(10, 6))
plt.bar(job_titles, percentage_match, color='salmon')
plt.xlabel('Job Title')
plt.ylabel('Percentage Match')
plt.title('Percentgae Match by Job Title')
plt.xticks(rotation=90)
plt.show()

"""### **MATCHING CONFIDENCE**"""

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

nltk.download('punkt')
nltk.download('stopwords')

def ngrams(string, n=3):
    string = fix_text(string)
    string = string.encode("ascii", errors="ignore").decode()
    string = string.lower()
    chars_to_remove = [")","(",".","|","[","]","{","}","'"]
    rx = '[' + re.escape(''.join(chars_to_remove)) + ']'
    string = re.sub(rx, '', string)
    string = string.replace('&', 'and')
    string = string.replace(',', ' ')
    string = string.replace('-', ' ')
    string = string.title()
    string = re.sub(' +',' ',string).strip()
    string = ' '+ string +' '
    string = re.sub(r'[,-./]|\sBD',r'', string)
    ngrams = zip(*[string[i:] for i in range(n)])
    return [''.join(ngram) for ngram in ngrams]

vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams, lowercase=False)
tfidf = vectorizer.fit_transform(skills)

from sklearn.neighbors import NearestNeighbors
nbrs = NearestNeighbors(n_neighbors=1, n_jobs=-1).fit(tfidf)
test = (df['test'].values.astype('U'))

def getNearestN(query):
  queryTFIDF_ = vectorizer.transform(query)
  distances, indices = nbrs.kneighbors(queryTFIDF_)
  return distances, indices

distances, indices = getNearestN(test)
test = list(test)
matches = []

for i,j in enumerate(indices):
    dist=round(distances[i][0],2)

    temp = [dist]
    matches.append(temp)

matches = pd.DataFrame(matches, columns=['Confidence'])

print(matches)

stemmer = PorterStemmer()
df['preprocessed'] = df['test'].fillna('').apply(lambda x: ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(str(x).lower()) if word not in stopw and len(word) > 2]))

resume_skills2 = ' '.join(skills)
resume_skills2 = ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(resume_skills2.lower()) if word not in stopw and len(word) > 2])

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)
tfidf_job_descriptions = vectorizer.fit_transform(df['preprocessed'])
tfidf_resume_skills = vectorizer.transform([resume_skills2])

cosine_similarities = cosine_similarity(tfidf_resume_skills, tfidf_job_descriptions).flatten()

percentage_matches = cosine_similarities * 100

job_matches = pd.DataFrame({
    'Job Id': df['Job Id'].values,
    'Work Type': df['Work Type'].values,
    'Job Title': df['Job Title'].values,
    'Match Confidence': percentage_matches
})

top_matches_with_details = job_matches.copy()

print(top_matches_with_details.columns)

print(top_matches_with_details[['Job Title', 'Work Type', 'Match Confidence']].reset_index(drop=True))

print("\nTop 20 Job Matches:")
top_20_matches = job_matches[['Job Id', 'Match Confidence']].nlargest(20, 'Match Confidence').reset_index(drop=True)

top_20_matches_with_details = pd.merge(top_20_matches, jobs_df, on='Job Id', how='left')

print(top_20_matches_with_details[['Job Title', 'Work Type', 'Match Confidence']].reset_index(drop=True))

jobs_df_updated = jobs_df.copy()

job_skills_counts = []
matched_skills_counts = []

for idx, row in jobs_df.iterrows():
    job_skills = row['Job Skills'].split(", ")
    matched_skills = [skill for skill in job_skills if skill.lower() in resume_skills2.split()]

    job_skills_counts.append(len(job_skills))
    matched_skills_counts.append(len(matched_skills))

jobs_df_updated['Total Skills Count'] = job_skills_counts
jobs_df_updated['Matched Skills Count'] = matched_skills_counts

print(jobs_df_updated.columns)

for job_id, match_confidence in zip(top_matches_with_details['Job Id'], top_matches_with_details['Match Confidence']):
    jobs_df_updated.loc[jobs_df_updated['Job Id'] == job_id, 'Match Confidence'] = match_confidence


print(jobs_df_updated[['Job Title', 'Work Type', 'Match Confidence']].reset_index(drop=True))

top_matched_skills = jobs_df_updated.sort_values(by=[ 'Match Confidence'], ascending=False).head(300)


print("\nTop 20 Highly Matched Skills Count with Total Skills Count and Match Confidence:")
print(top_matched_skills[['Job Title', 'Work Type', 'Total Skills Count', 'Matched Skills Count', 'Match Confidence']].reset_index(drop=True))

"""### **PIE CHART BETWEEN JOB TITLE AND MATCH CONFIDENCE**"""

job_titles = top_matched_skills['Job Title']
total_skills_count = top_matched_skills['Match Confidence']

import pandas as pd
import matplotlib.pyplot as plt

grouped_skills_df = top_matched_skills.groupby('Job Title')['Match Confidence'].sum().reset_index()

job_titles = grouped_skills_df['Job Title']
total_skills_count = grouped_skills_df['Match Confidence']

plt.figure(figsize=(8, 8))
plt.pie(total_skills_count, labels=job_titles, autopct='%1.1f%%', startangle=140)
plt.axis('equal')
plt.title('Distribution of Match Confidence')
plt.show()

"""### **BAR CHART BETWEEN JOB TITLE AND TOTAL SKILLS COUNT**"""

import matplotlib.pyplot as plt

job_titles = top_matched_skills['Job Title']
total_skills_count = top_matched_skills['Total Skills Count']
matched_skills_count = top_matched_skills['Matched Skills Count']
percentage_match = top_matched_skills['Match Confidence']


plt.figure(figsize=(10, 6))
plt.bar(job_titles, total_skills_count, color='skyblue')
plt.xlabel('Job Title')
plt.ylabel('Total Skills Count')
plt.title('Total Skills Count by Job Title')
plt.xticks(rotation=90)
plt.show()

"""### **BAR CHART BETWEEN JOB TITLE AND MATCHED SKILLS COUNT**"""

plt.figure(figsize=(10, 6))
plt.bar(job_titles, matched_skills_count, color='lightgreen')
plt.xlabel('Job Title')
plt.ylabel('Matched Skills Count')
plt.title('Matched Skills Count by Job Title')
plt.xticks(rotation=90)
plt.show()

"""### **BAR CHART BETWEEN JOB TITLE AND MATCH CONFIDENCE**"""

plt.figure(figsize=(10, 6))
plt.bar(job_titles, percentage_match, color='salmon')
plt.xlabel('Job Title')
plt.ylabel('Match Confidence')
plt.title('Match Confidence by Job Title')
plt.xticks(rotation=90)
plt.show()

"""### **QUANTIFIABLE ACHIEVEMENTS TO MATCH RESUME WITH JOB DESCRIPTION USING KNN ALOGORITHM**"""

import os
import pandas as pd
from PyPDF2 import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

def extract_text_from_pdf(file_path):
    with open(file_path, 'rb') as f:
        reader = PdfReader(f)
        text = ''
        for page in reader.pages:
            text += page.extract_text()
    return text

jobs_df = pd.read_csv('/content/modified_jobs.csv')

resume_folder = '/content/resumesfolder'
resume_files = [os.path.join(resume_folder, file) for file in os.listdir(resume_folder) if file.endswith('.pdf')]
resumes_text = [extract_text_from_pdf(file) for file in resume_files]

features = df[['Experience', 'Job Title', 'Work Type', 'Qualifications']]
labels = df['Job Title']

vectorizer = TfidfVectorizer()
X_features = vectorizer.fit_transform(features.apply(lambda x: ' '.join(x), axis=1))
X_resumes = vectorizer.transform(resumes_text)

X_train, X_test, y_train, y_test = train_test_split(X_features, labels, test_size=0.2, random_state=42)

knn_classifier = KNeighborsClassifier()
knn_classifier.fit(X_train, y_train)

predicted_job_titles = knn_classifier.predict(X_resumes)

resume_list = []
job_title_list = []
job_id_list = []
experience_list = []
work_type_list = []
qualifications_list = []

for resume, predicted_job_title in zip(resume_files, predicted_job_titles):
    job_index = jobs_df[jobs_df['Job Title'] == predicted_job_title].index[0]
    job_id = jobs_df.at[job_index, 'Job Id']
    experience = jobs_df.at[job_index, 'Experience']
    job_title = jobs_df.at[job_index, 'Job Title']
    work_type = jobs_df.at[job_index, 'Work Type']
    qualifications = jobs_df.at[job_index, 'Qualifications']

    resume_list.append(resume)
    job_title_list.append(predicted_job_title)
    job_id_list.append(job_id)
    experience_list.append(experience)
    work_type_list.append(work_type)
    qualifications_list.append(qualifications)

matched_results_df = pd.DataFrame({
    'Resume': resume_list,
    'Job Title': job_title_list,
    'Job ID': job_id_list,
    'Experience': experience_list,
    'Work Type': work_type_list,
    'Qualifications': qualifications_list
})

print(matched_results_df)

y_pred = knn_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

accuracy_percentage = accuracy * 100
print(f"Accuracy: {accuracy_percentage:.2f}%")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")

print(classification_report(y_test, y_pred))

"""### **QUANTIFIABLE ACHIEVEMENTS TO MATCH RESUME WITH JOB DESCRIPTION USING SVM ALOGORITHM**"""

import os
import pandas as pd
from PyPDF2 import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

def extract_text_from_pdf(file_path):
    with open(file_path, 'rb') as f:
        reader = PdfReader(f)
        text = ''
        for page in reader.pages:
            text += page.extract_text()
    return text

jobs_df = pd.read_csv('/content/modified_jobs.csv')

resume_folder = '/content/resumesfolder'
resume_files = [os.path.join(resume_folder, file) for file in os.listdir(resume_folder) if file.endswith('.pdf')]
resumes_text = [extract_text_from_pdf(file) for file in resume_files]

features = jobs_df[['Experience', 'Job Title', 'Work Type', 'Qualifications']]
labels = jobs_df['Job Title']

vectorizer = TfidfVectorizer()
X_features = vectorizer.fit_transform(features.apply(lambda x: ' '.join(x), axis=1))
X_resumes = vectorizer.transform(resumes_text)

X_train, X_test, y_train, y_test = train_test_split(X_features, labels, test_size=0.2, random_state=42)

svm_classifier = SVC()
svm_classifier.fit(X_train, y_train)

predicted_job_titles_svm = svm_classifier.predict(X_resumes)

resume_list_svm = []
job_title_list_svm = []
job_id_list_svm = []
experience_list_svm = []
work_type_list_svm = []
qualifications_list_svm = []

for resume, predicted_job_title in zip(resume_files, predicted_job_titles_svm):
    if predicted_job_title is None:

        resume_list_svm.append(resume)
        job_title_list_svm.append(None)
        job_id_list_svm.append(None)
        experience_list_svm.append(None)
        work_type_list_svm.append(None)
        qualifications_list_svm.append(None)
    else:
        job_index = jobs_df[jobs_df['Job Title'] == predicted_job_title].index[0]
        job_id = jobs_df.at[job_index, 'Job Id']
        experience = jobs_df.at[job_index, 'Experience']
        job_title = jobs_df.at[job_index, 'Job Title']
        work_type = jobs_df.at[job_index, 'Work Type']
        qualifications = jobs_df.at[job_index, 'Qualifications']

        resume_list_svm.append(resume)
        job_title_list_svm.append(predicted_job_title)
        job_id_list_svm.append(job_id)
        experience_list_svm.append(experience)
        work_type_list_svm.append(work_type)
        qualifications_list_svm.append(qualifications)

matched_results_df_svm1 = pd.DataFrame({
    'Resume': resume_list_svm,
    'Job Title': job_title_list_svm,
    'Job ID': job_id_list_svm,
    'Experience': experience_list_svm,
    'Work Type': work_type_list_svm,
    'Qualifications': qualifications_list_svm
})

print("Matched Results using SVM:")
print(matched_results_df_svm1)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

y_pred_svm = svm_classifier.predict(X_test)

accuracy_svm = accuracy_score(y_test, y_pred_svm)
precision_svm = precision_score(y_test, y_pred_svm, average='weighted')
recall_svm = recall_score(y_test, y_pred_svm, average='weighted')
f1_svm = f1_score(y_test, y_pred_svm, average='weighted')

accuracy_percentage_svm = accuracy_svm * 100

print("Evaluation Metrics for SVM:")
print(f"Accuracy: {accuracy_percentage_svm:.2f}%")
print(f"Precision: {precision_svm}")
print(f"Recall: {recall_svm}")
print(f"F1-score: {f1_svm}")

print("Detailed classification report for SVM:")
print(classification_report(y_test, y_pred_svm))